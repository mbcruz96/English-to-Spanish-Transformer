{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8W1AiWnCgUTw",
        "hZvx0Hizsbbm",
        "P2cI5_R85J9X",
        "sIP3vGMZgYqt",
        "0hmkyXWxgcoz",
        "GVCEYiEr4eF2"
      ],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMO5k7mIPqUKS2yL1yuhCZU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbcruz96/English-to-Spanish-Transformer/blob/main/English_to_Spanish_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Libraries"
      ],
      "metadata": {
        "id": "8W1AiWnCgUTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install tokenizers\n",
        "!pip install tqdm\n",
        "!pip install tensorboard\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UtBhCHg5nAa",
        "outputId": "fc7a76db-a890-43c4-e3fa-e81066011e1c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.0.post0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dependencies"
      ],
      "metadata": {
        "id": "hZvx0Hizsbbm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LL-PBlpYHQVn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Any\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchmetrics\n",
        "from torchmetrics.text import CharErrorRate, WordErrorRate, BLEUScore\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mounting Drive"
      ],
      "metadata": {
        "id": "P2cI5_R85J9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3ksSZoi5MTs",
        "outputId": "e2567aad-8db2-4186-d626-8114344e0b77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "sIP3vGMZgYqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  # embedding dimensionality\n",
        "        self.vocab_size = vocab_size    # size of corpus vocabulary\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)  # learnable vobabulary embedding\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, sequence_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.sequence_len = sequence_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # creating a positional encoding matrix of dimension (sequence length, embedding dimensionality)\n",
        "        pos_enc = torch.zeros(sequence_len, d_model)\n",
        "        # creating positions vector of dimenstion (sequence length, 1)\n",
        "        positions = torch.arange(0, sequence_len, dtype=torch.float).unsqueeze(1)\n",
        "        divisor = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pos_enc[:, ::2] = torch.sin(positions * divisor)\n",
        "        pos_enc[:, 1::2] = torch.cos(positions * divisor)\n",
        "\n",
        "        # creating a positional encoding for a batch of embeddings of size (1, sequence length, embedding dimensionality)\n",
        "        pos_enc = pos_enc.unsqueeze(0)\n",
        "\n",
        "        # saving positional encoding as non-learnable parameter in same file as model weights\n",
        "        self.register_buffer('pos_enc', pos_enc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pos_enc[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class LayerNormalizationBlock(nn.Module):\n",
        "    def __init__(self, eps: float = 10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.gamma = nn.Parameter(torch.ones(1, device='cuda')) # Multiplicative parameter\n",
        "        self.beta = nn.Parameter(torch.zeros(1, device='cuda')) # Addative parameter\n",
        "\n",
        "    # input: (batch_size, seq_len, emb_dim)\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.layer1 = nn.Linear(d_model, d_ff).to(self.device)  # fc layer of dimension (ff_dim, emb_dim)\n",
        "        self.layer2 = nn.Linear(d_ff, d_model).to(self.device)  # fc layer of dimension (emb_dim, ff_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input: (batch_size, seq_len, emb_dim, ff_dim) --> (batch_size, seq_len, ff_dim, emb_dim) --> (batch_size, seq_len, emb_dim, ff_dim)\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.layer2(x)\n",
        "\n",
        "class MultiHeadSelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        assert d_model % h == 0, 'Embedding dimensionality not divisible by number of heads'\n",
        "        self.d_k = d_model // h\n",
        "\n",
        "        # learnable weight matrices\n",
        "        self.w_q = nn.Linear(d_model, d_model).to(self.device)\n",
        "        self.w_k = nn.Linear(d_model, d_model).to(self.device)\n",
        "        self.w_v = nn.Linear(d_model, d_model).to(self.device)\n",
        "        self.w_o = nn.Linear(d_model, d_model).to(self.device)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "\n",
        "        # input: (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask == 0, -1**9)\n",
        "        # (batch, h, seq_len, seq_len)\n",
        "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
        "        if dropout is not None:\n",
        "            dropout(attention_scores)\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        # input: (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        query = self.w_q(q)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "\n",
        "        # input: (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, self.attention_scores = MultiHeadSelfAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # input: (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k) # contiguous allows tensor shape to be changed in contiguous memory block\n",
        "\n",
        "        return self.w_o(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalizationBlock()\n",
        "\n",
        "    def forward(self, skip_layer, prev_layer):\n",
        "        return skip_layer + self.dropout(prev_layer(self.norm(skip_layer)))\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, attention_block: MultiHeadSelfAttentionBlock, ff_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.attention_block = attention_block\n",
        "        self.ff_block = ff_block\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.residual_connections = nn.ModuleList(ResidualConnection(dropout) for _ in range(2))\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.ff_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalizationBlock()\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, attention_block: MultiHeadSelfAttentionBlock, cross_attention_block: MultiHeadSelfAttentionBlock, ff_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.attention_block = attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.ff_block = ff_block\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.residual_connections = nn.ModuleList(ResidualConnection(dropout) for _ in range(3))\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.ff_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalizationBlock()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.projection = nn.Linear(d_model, vocab_size).to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input: (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return torch.log_softmax(self.projection(x), dim=-1) # log softmax for numerical stability\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_pos_enc: PositionalEncoding, tgt_pos_enc: PositionalEncoding, src_embed: TextEmbeddings, tgt_embed: TextEmbeddings, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pos_enc = src_pos_enc\n",
        "        self.tgt_pos_enc = tgt_pos_enc\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos_enc(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, tgt, encoder_output, src_mask, tgt_mask,):\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos_enc(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def projection(self, x):\n",
        "        return self.projection_layer(x)\n",
        "\n",
        "def BuildTransformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, h: int = 8, N: int = 6, d_ff: int = 2048, dropout: float = 0.1) -> Transformer:\n",
        "    # Create embedding layers\n",
        "    src_embedding = TextEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embedding = TextEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create positional encodings\n",
        "    src_pos_enc = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos_enc = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        attention_block = MultiHeadSelfAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        attention_block = MultiHeadSelfAttentionBlock(d_model, h, dropout)\n",
        "        cross_attention_block = MultiHeadSelfAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(attention_block, cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # create encoder and decoder\n",
        "    encoder = Encoder(encoder_blocks)\n",
        "    decoder = Decoder(decoder_blocks)\n",
        "    # create projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "    # create transformer\n",
        "    transformer = Transformer(encoder, decoder, src_pos_enc, tgt_pos_enc, src_embedding, tgt_embedding, projection_layer)\n",
        "    # Initialize transformer parameters for better learning\n",
        "    for layer in transformer.parameters():\n",
        "        if layer.dim() > 1:\n",
        "            nn.init.xavier_uniform_(layer)\n",
        "\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "ubGAzOzmgamg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "0hmkyXWxgcoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BilingualDataset(Dataset):\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
        "        super().__init__()\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_src.token_to_id('[PAD]')], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, index: Any) -> Any:\n",
        "        src_target_pair = self.ds[index]\n",
        "        src_txt = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_txt = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_txt).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_txt).ids\n",
        "\n",
        "        enc_num_pad_tokens = self.seq_len - len(enc_input_tokens) - 2\n",
        "        dec_num_pad_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        if enc_num_pad_tokens < 0:\n",
        "            enc_input_tokens = enc_input_tokens[:self.seq_len - 2]\n",
        "            enc_num_pad_tokens = 0\n",
        "        if dec_num_pad_tokens < 0:\n",
        "            dec_input_tokens = dec_input_tokens[:self.seq_len - 1]\n",
        "            dec_num_pad_tokens = 0\n",
        "\n",
        "        # Add SOS and EOS to source text\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_pad_tokens, dtype=torch.int64)\n",
        "            ],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        # Add SOS to decoder input\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_pad_tokens, dtype=torch.int64)\n",
        "            ],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        # Add SOS to the label (expected decoder output)\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_pad_tokens, dtype=torch.int64)\n",
        "            ],\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        # Check to ensure the sizes of encoder, decoder, and label texts are of sequence length\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        # data set\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input, # size = seq_len\n",
        "            \"decoder_input\": decoder_input, # size = seq_len\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & Causual_Mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len)\n",
        "            \"label\": label, # (seq_len)\n",
        "            \"source_text\": src_txt,\n",
        "            \"target_text\": tgt_txt\n",
        "        }\n",
        "\n",
        "# returns a masked matrix with all values above diagonal masked out\n",
        "def Causual_Mask(size):\n",
        "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n",
        ""
      ],
      "metadata": {
        "id": "QjLcHD_Zgivi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Configurations"
      ],
      "metadata": {
        "id": "wWOgZ7vAgsl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Get_Config():\n",
        "    return{\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 20,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 350,\n",
        "        \"d_model\": 512,\n",
        "        \"lang_src\": \"en\",\n",
        "        \"lang_tgt\": \"es\",\n",
        "        \"model_folder\": \"weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"drive_path\": \"/content/drive/MyDrive/Colab/TranslationTransformer/\",\n",
        "        \"preload\": \"latest\",\n",
        "        \"tokenizer_filename\": \"tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/tmodel\"\n",
        "    }\n",
        "\n",
        "def Get_Weights_File_Path(config, epoch: str):\n",
        "    drive_path = config['drive_path']\n",
        "    model_folder = config['model_folder']\n",
        "    model_basename = config['model_basename']\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
        "    return str(Path('.') / drive_path / model_folder / model_filename)\n",
        "\n",
        "def Get_Tokenizer_File_Path(config, lang: str):\n",
        "    drive_path = config['drive_path']\n",
        "    tokenizer_filename = Path(config['tokenizer_filename'].format(lang))\n",
        "    return str(Path('.') / drive_path / tokenizer_filename)\n",
        "\n",
        "def Get_Model_File_Path(config):\n",
        "    drive_path = config['drive_path']\n",
        "    model_folder = config['model_folder']\n",
        "    return str(Path('.') / drive_path / model_folder)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def Latest_Weights_File_Path(config):\n",
        "    model_folder = f\"{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ],
      "metadata": {
        "id": "qZtmlhdNgwuW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference"
      ],
      "metadata": {
        "id": "GVCEYiEr4eF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Greedy_Decode(model, device, encoder_input, encoder_mask, src_tokenizer, tgt_tokenizer, max_len):\n",
        "    sos_idx = tgt_tokenizer.token_to_id('[SOS]')\n",
        "    eos_idx = tgt_tokenizer.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute encoder output and reuse it for every token we get from the decoder\n",
        "    encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "    # Initialize the decoder output with the SOS token\n",
        "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(encoder_input).to(device)\n",
        "\n",
        "    while True:\n",
        "        # Break if decoder is max sequence length\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for the current decoder input\n",
        "        decoder_mask = Causual_Mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n",
        "\n",
        "        # calculate the decoder output\n",
        "        output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)\n",
        "\n",
        "        # get the next token\n",
        "        prob = model.projection(output[:, -1]) # project the output for the last token in the sequence\n",
        "        # select the token with the next highest probability\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1,1).type_as(encoder_input).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "        # break if next word is eos token\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    # return generated sequence without batch dimension\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "def Run_Validation(model, device, val_ds, src_tokenizer, tgt_tokenizer, max_len, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    console_width = 80 # size of control window\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "\n",
        "            # ensure validation batch is 1\n",
        "            assert encoder_input.size(0) == 1, 'Batch size must be 1 for validation'\n",
        "\n",
        "            # generate the output tokens\n",
        "            model_out = Greedy_Decode(model, device, encoder_input, encoder_mask, src_tokenizer, tgt_tokenizer, max_len)\n",
        "\n",
        "            # get actual model output\n",
        "            source_text = batch['source_text'][0]\n",
        "            target_text = batch['target_text'][0]\n",
        "            model_out_text = tgt_tokenizer.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "             # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "            if writer:\n",
        "                # Evaluate the character error rate\n",
        "                # Compute the char error rate\n",
        "                metric = torchmetrics.CharErrorRate()\n",
        "                cer = metric(model_out_text, target_text)\n",
        "                writer.add_scalar('validation cer', cer, global_step)\n",
        "                writer.flush()\n",
        "\n",
        "                # Compute the word error rate\n",
        "                metric = torchmetrics.WordErrorRate()\n",
        "                wer = metric(model_out_text, target_text)\n",
        "                writer.add_scalar('validation wer', wer, global_step)\n",
        "                writer.flush()\n",
        "\n",
        "                # Compute the BLEU metric\n",
        "                metric = torchmetrics.BLEUScore()\n",
        "                bleu = metric(model_out_text, target_text)\n",
        "                writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "                writer.flush()"
      ],
      "metadata": {
        "id": "HT9IM26I4gTQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "acoUP4UKg3CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Get_All_Sentances(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]\n",
        "\n",
        "def Get_or_Build_Tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Get_Tokenizer_File_Path(config, lang)\n",
        "    if not Path(tokenizer_path).exists():\n",
        "        print('Tokenizer not found, building...')\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = WordLevelTrainer(special_tokens=['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency=2)\n",
        "        tokenizer.train_from_iterator(Get_All_Sentances(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        print('Tokenizer found, loading...')\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n",
        "def Get_Dataset(config):\n",
        "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
        "\n",
        "    # Built tokenizer\n",
        "    tokenizer_src = Get_or_Build_Tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = Get_or_Build_Tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Split train and validation sets\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) -  train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentance: {max_len_src}')\n",
        "    print(f'Max length of target sentance: {max_len_tgt}')\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n",
        "def Get_Model(config, src_vocab_size, tgt_vocab_size):\n",
        "    model = BuildTransformer(src_vocab_size, tgt_vocab_size, config['seq_len'], config['seq_len'], config['d_model'])\n",
        "    return model\n",
        "\n",
        "def Train_model(config):\n",
        "    # setting device to train on\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device {device}')\n",
        "\n",
        "    # Create file for parameters to be saved to\n",
        "    model_folder = Get_Model_File_Path(config)\n",
        "    Path(model_folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Initialize dataloaders and tokenizers\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = Get_Dataset(config)\n",
        "\n",
        "    # Initialize model\n",
        "    model = Get_Model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "    # Create a tensorboard for loss visualization\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    # Initial epoch start and global stop of this training cycle\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # preload previous model parameters if they exist\n",
        "    if config['preload'] == \"latest\":\n",
        "        model_filename = Latest_Weights_File_Path(config)\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)  # loading previous model state\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "        print(f'Starting training at epoch {initial_epoch}')\n",
        "    else:\n",
        "      model_filename = Get_Model_File_Path(config, config['preload'])\n",
        "      print(f'No model found to preload')\n",
        "\n",
        "    # Initializing loss function\n",
        "    # Ignores any padded tokens in loss computation\n",
        "    # Smoothes result by distributing .1 of argmax value to all other labels\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        # Initialize batch iterator using tqdm for a progression bar visualization\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
        "        for batch in batch_iterator:\n",
        "            if batch == None:\n",
        "                continue\n",
        "            encoder_input = batch['encoder_input'].to(device)   # input: (batch, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device)   # input: (batch, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)     # (batch, 1, 1, seq_len) to mask padded tokens\n",
        "            decoder_mask = batch['decoder_mask'].to(device)     # (batch, 1, seq_len, seq_len) to mask padded tokens and future words\n",
        "\n",
        "            # run tensors through model\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)  # (batch, seq_len, d_model)\n",
        "            decoder_output = model.decode(decoder_input, encoder_output, encoder_mask, decoder_mask)    # (batch, seq_len, d_model)\n",
        "            projection_output = model.projection(decoder_output)    # (batch, seq_len, tgt_vocab_size)\n",
        "            # Get label from batch\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            # input (batch, seq_len, tgt_vocab_size) --> (batch * seq_len, tgt_vocab_size)\n",
        "            loss = loss_fn(projection_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "\n",
        "            # update the progress bar with the loss\n",
        "            batch_iterator.set_postfix({f'loss': f'{loss.item():6.3f}'})\n",
        "\n",
        "            # log the loss in tensorboard\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Backpropogate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = Get_Weights_File_Path(config, f'{epoch:02d}')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)\n",
        "\n",
        "        # Run validation at the end of each epoch\n",
        "        Run_Validation(model, device, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], lambda msg: batch_iterator.write(msg), global_step, writer)"
      ],
      "metadata": {
        "id": "ZHVIuO16g2mN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "BhcrbirVPBnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = Get_Config()\n",
        "Train_model(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCOa-K8xg-6i",
        "outputId": "3698dc28-004e-4800-9b0a-77ad337fe29c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n",
            "Tokenizer found, loading...\n",
            "Tokenizer found, loading...\n",
            "Max length of source sentance: 767\n",
            "Max length of target sentance: 782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 00: 100%|| 10516/10516 [26:30<00:00,  6.61it/s, loss=4.629]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: How could you have consoled her!--I cannot express my own abhorrence of myself.\n",
            "    TARGET: Cmo habra perdurado en tus recuerdos! Y mi madre, tambin!\n",
            " PREDICTED: - Pero , que la seora , que la que la que la que la que la , que la , que la , que la , que la , que la , que la , que la , que , que , que , que , que no , que , , , , que , , , , que , , , , , , , , , , , , , que , , , , , , , , , , de\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `CharErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `CharErrorRate` from `torchmetrics.text` instead.\n",
            "  _future_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `WordErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `WordErrorRate` from `torchmetrics.text` instead.\n",
            "  _future_warning(\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `BLEUScore` from `torchmetrics` was deprecated and will be removed in 2.0. Import `BLEUScore` from `torchmetrics.text` instead.\n",
            "  _future_warning(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: And the Eaglet bent down its head to hide a smile: some of the other birds tittered audibly.\n",
            "    TARGET: Y el Aguilucho baj la cabeza para ocultar una sonrisa; algunos de los otros pjaros rieron sin disimulo.\n",
            " PREDICTED: Y , que la , que la , que la , , que la , , , , , ,\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 01: 100%|| 10516/10516 [26:31<00:00,  6.61it/s, loss=5.868]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"So, as I was saying,\" continued Sancho, \"as the pair of them were going to sit down to table, as I said, the labourer insisted upon the gentleman's taking the head of the table, and the gentleman insisted upon the labourer's taking it, as his orders should be obeyed in his house; but the labourer, who plumed himself on his politeness and good breeding, would not on any account, until the gentleman, out of patience, putting his hands on his shoulders, compelled him by force to sit down, saying, 'Sit down, you stupid lout, for wherever I sit will be the head to you; and that's the story, and, troth, I think it hasn't been brought in amiss here.\"\n",
            "    TARGET: -Digo, as -dijo Sancho-, que, estando, como he dicho, los dos para sentarse a la mesa, el labrador porfiaba con el hidalgo que tomase la cabecera de la mesa, y el hidalgo porfiaba tambin que el labrador la tomase, porque en su casa se haba de hacer lo que l mandase; pero el labrador, que presuma de corts y bien criado, jams quiso, hasta que el hidalgo, mohno, ponindole ambas manos sobre los hombros, le hizo sentar por fuerza, dicindole: ''Sentaos, majagranzas, que adondequiera que yo me siente ser vuestra cabecera''. Y ste es el cuento, y en verdad que creo que no ha sido aqu trado fuera de propsito.\n",
            " PREDICTED: - Y , Sancho , seor , Sancho , Sancho , y , y , a la venta , y , a la mesa , y , y , y , que , y , como si , y , y , como , como si , y , como si , y , y , y , como si , como si , como si , como si , como si , como si , como si se le dijo , como si , como si , como si , y , como si , como si no , y , como si , que , y , que , como si , que , y , como si , como si , que , como si , como si , como si , como si , que , que , si , que , que , como si no se le dijo , que , que , que , que , como si el cura , que , como si no se puso en el cura , que , que , como si no se ha dicho , que , seor , como si se ha dicho , como vuestra merced , que , como si no se ha dicho , seor , seor don Quijote , Sancho , seor don Quijote , que , que , que , que , que , que , como si no se puso a Sancho , seor , seor don Quijote , que , que , que , seor , que , y , seor , y , seor don Quijote , seor , que , que , que , y , y , y , y , que no le dijo , que , no se puso en el cual , que no se le dijo , seor don Quijote , no se le dijo , y , no se le dijo don Quijote -, que se le dijo don Quijote , y , y , que no se le dijo don Quijote , y , que , seor\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Somewhere there, on that desolate plain, was lurking this fiendish man, hiding in a burrow like a wild beast, his heart full of malignancy against the whole race which had cast him out.\n",
            "    TARGET: En algn lugar de aquella llanura desolada se esconda el diablico asesino, oculto en un escondrijo como una bestia salvaje y con el corazn lleno de malevolencia hacia toda la raza humana que lo haba expulsado de su seno.\n",
            " PREDICTED: En aquel momento en el que el que haba hecho un hombre que haba hecho en el hombre que haba hecho en el hombre que haba hecho en el hombre que haba hecho un hombre que haba hecho en el que haba hecho en el hombre que haba hecho en el cual haba hecho en el hombre que haba hecho en el que haba hecho en el hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho en el hombre que haba hecho un hombre que haba hecho en el hombre que haba hecho en el cual haba hecho un hombre que haba hecho un hombre que haba hecho en el hombre que haba hecho en el hombre que haba hecho en el hombre que haba hecho un hombre que haba hecho un hombre que haba hecho en el hombre que haba hecho un hombre que el hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho en el hombre que haba hecho un hombre que haba hecho un hombre que haba hecho en un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre hombre que haba hecho un hombre que haba hecho en el hombre que haba hecho en el hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba hecho un hombre que haba\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 02: 100%|| 10516/10516 [26:24<00:00,  6.64it/s, loss=5.288]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Cyrus Harding looked at his dog, and those of his companions who were near him might have heard him murmur these words,--\n",
            "    TARGET: Ciro Smith miraba a Top y, si alguno de sus compaeros se hubiera acercado al ingeniero en aquel momento, le habra odo murmurar:\n",
            " PREDICTED: Ciro Smith y Ciro Smith y Ciro Smith y Ciro Smith y Ciro Smith y Ciro Smith y Ciro Smith y Ciro Smith y Ciro Smith y Ciro Smith y sus compaeros , y Ciro Smith y sus compaeros , y Ciro Smith y sus compaeros , y sus compaeros , y sus compaeros , y sus compaeros , y sus compaeros , Ciro Smith y sus compaeros , y sus compaeros , y sus compaeros , y sus compaeros , y sus compaeros y sus compaeros , y sus compaeros , Ciro Smith y sus compaeros , y sus compaeros , y , y sus compaeros , y sus compaeros , y sus compaeros , y sus compaeros , y sus compaeros y sus compaeros , Ciro Smith , y sus compaeros , y sus compaeros , y sus compaeros , y sus compaeros , y sus compaeros , y sus compaeros , Ciro Smith y que Ciro Smith y sus compaeros , sus compaeros , Ciro Smith y , y sus compaeros , Ciro Smith y sus compaeros , Ciro Smith , Ciro Smith y sus compaeros , Ciro Smith y sus compaeros , sus compaeros , y sus compaeros , Ciro Smith y sus compaeros se haba estado en sus compaeros , Ciro Smith y Ciro Smith y Ciro Smith y Ciro Smith y Ciro Smith y sus compaeros , Ciro Smith , Ciro Smith y sus compaeros , Ciro Smith , Ciro Smith y sus compaeros , Ciro Smith y Ciro Smith y los que Ciro Smith y Ciro Smith y Ciro Smith , Ciro Smith y Ciro Smith , Ciro Smith y Ciro Smith y Ciro Smith y sus compaeros , y sus compaeros , Ciro Smith y no se haban vuelto a Ciro Smith y Ciro Smith , Ciro Smith , Ciro Smith , Ciro Smith y sus compaeros , no se haba estado de Ciro Smith y Ciro Smith y Ciro Smith y sus compaeros , Ciro Smith , Ciro Smith , Ciro Smith , Ciro Smith , Ciro Smith , Ciro\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Yes, my boy.\n",
            "    TARGET: -En efecto.\n",
            " PREDICTED: S , s , s , s , lo que , lo que lo que , lo que , lo que lo que lo que lo que le dijo : - S , lo que lo que lo que lo que lo que lo que lo que le haba hecho lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que le haba hecho lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que le haba hecho lo que lo que lo que lo que lo que lo que es lo que es lo que es que es lo que lo que lo que lo que lo que lo que lo que lo que lo que es que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que s mismo que lo que lo que lo que lo que lo que lo que lo que lo que lo que lo que es lo que mi lado es mi vida .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 03: 100%|| 10516/10516 [26:30<00:00,  6.61it/s, loss=4.824]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: 'Where is the humiliation?\n",
            "    TARGET: Dnde est la ofensa?\n",
            " PREDICTED:  Y qu es decir ,  Dnde est ?  dijo :  Y quin ?  pregunt a la  Y quin ?  pregunt el  Y qu ?\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Well, if you are cold, Planchet, you can go into one of those cabarets that you see yonder, and be in waiting for me at the door by six oclock in the morning.\"\n",
            "    TARGET: Bueno, si tienes fro, Planchet, entra en una de esas tabernas que ves all abajo, y me esperas maana a las seis delante de la puerta.\n",
            " PREDICTED:  Pues bien , que , que , que , que , que , que , que , que , que se ha dicho que , que , que ha dicho , que se ha de que , que se ha dicho , que , que se ha de la puerta , que se ha de que , que ha de la puerta , que vos , que , que ha de que se ha de la maana , que no se ha dicho , que se ha dicho , que no se ha venido a la maana , que vos , que se ha dicho , que se ha dicho que vos , que no se ha de la maana , que se ha dicho , que se ha dicho , que se ha dicho que se ha dicho que se ha pasado maana , que os ha dicho que se ha dicho que se ha de la puerta , que ha dicho que ha dicho que ha dicho , que ha pasado , que vos , que ha pasado , seor , seor , que ha dicho , seor , que ha de la maana , que ha pasado maana , que ha pasado maana os ha pasado maana , que vos habis de la maana , seor , seor Willoughby , seor , que ha pasado maana , seor , que ha pasado seis de las diez de las seis , seor , seor Willoughby , seor Willoughby , que no menos que se ha dicho que no se ha pasado maana , seor , seor cardenal , seor , se ha dicho , seor , que no se ha de las seis , seor , seor , no ha venido a vuestra merced , seor D ' Artagnan no ha vuelto a la maana , seor , no est en la maana , no se ha pasado maana os ha salido a la puerta , seor D ' Artagnan , seor , seor , seor Willoughby , que no ha pasado maana , no ha\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 04: 100%|| 10516/10516 [26:23<00:00,  6.64it/s, loss=4.920]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Being much less cool-headed than Mr. Fogg, he was much more restless, counting and recounting the days passed over, uttering maledictions when the train stopped, and accusing it of sluggishness, and mentally blaming Mr. Fogg for not having bribed the engineer.\n",
            "    TARGET: Contaba y volva a contar los das transcurridos, maldeca las paradas del tren, lo acusaba de lentitud y vituperaba \"in pectore\" a mister Fogg por no haber prometido una prima al maquinista. No saba el buen muchacho que lo que era posible en un vapor no tena aplicacin en un ferrocarril, cuya velocidad era reglamentaria.\n",
            " PREDICTED: A pesar de Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , y , y Phileas Fogg , y Phileas Fogg , Phileas Fogg , y Phileas Fogg , seor Phileas Fogg , seor Fix , y , seor Fix , seor Fix , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Fix , seor Phileas Fogg , mister Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , y Phileas Fogg , mister Fogg , seor Phileas Fogg , seor Phileas Fogg , mister Fogg , y Phileas Fogg , mister Fogg , Phileas Fogg , mister Fogg , mister Fogg , seor Phileas Fogg , y , y Phileas Fogg , seor Phileas Fogg , mister Fogg , mister Fogg , seor Phileas Fogg , mister Fogg , y Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg y Phileas Fogg , mister Fogg , seor Fix , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Phileas Fogg , seor Phileas Fogg , seor Fix , seor Fix , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Fogg , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Phileas Fogg , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Fix , seor Phileas Fogg , seor Fix , seor Fix , seor Phileas Fogg , seor Phileas Fogg , seor Phileas Fogg , seor Fix , seor Phileas Fogg , seor\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: And he got up very sulkily and crossed over to the other side of the court.\n",
            "    TARGET: Se levant con aire digno y fue a situarse al otro extremo de la sala.\n",
            " PREDICTED: Y , Y , que el otro lado , el otro , que el otro lado , que el otro lado , que se puso a la orden de la que se puso a la otra vez , se haba hecho de la habitacin , se puso a la otra cosa , se , se , se ha venido a la orden de la calle , se , y se , se ha hecho de la sala de la corte .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 05: 100%|| 10516/10516 [26:26<00:00,  6.63it/s, loss=4.636]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I went to my window, opened it, and looked out.\n",
            "    TARGET: Abr la ventana y mir al exterior.\n",
            " PREDICTED: , mir a la ventana , y mir a la ventana , y , y a la ventana , y a la ventana , y , y a la ventana , y a la ventana , y a la ventana , y , y a la ventana , y , y me abri y , y , y , y , y , y , y , y me abri y , y me hizo entrar en la ventana , y me hizo .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"I say then,\" continued Sancho, \"that in a village of Estremadura there was a goat-shepherdthat is to say, one who tended goatswhich shepherd or goatherd, as my story goes, was called Lope Ruiz, and this Lope Ruiz was in love with a shepherdess called Torralva, which shepherdess called Torralva was the daughter of a rich grazier, and this rich grazier-\"\n",
            "    TARGET: -Digo, pues -prosigui Sancho-, que en un lugar de Estremadura haba un pastor cabrerizo (quiero decir que guardaba cabras), el cual pastor o cabrerizo, como digo, de mi cuento, se llamaba Lope Ruiz; y este Lope Ruiz andaba enamorado de una pastora que se llamaba Torralba, la cual pastora llamada Torralba era hija de un ganadero rico, y este ganadero rico...\n",
            " PREDICTED: - As , Sancho - prosigui Sancho - respondi Sancho - respondi Sancho -, que fue a Sancho -, que fue Sancho - respondi Sancho - respondi Sancho - respondi Sancho , que fue Sancho , que fue a Sancho Panza , que fue a Sancho - respondi Sancho - que dice que fue a Sancho , que fue a Sancho - que fue a esta hermosa Dorotea -, que se , que , que fue a esta historia , que fue a esta historia , que fue una hermosa Dorotea , que fue el cabrero - que fue una hermosa Dorotea , que fue hermosa Dorotea , que fue una historia que este tiempo que fue a esta sazn Sancho Panza , que fue desta doncella , que es - que este tiempo que fue una hija , que fue , que fue a Sancho - que este tiempo que dice - que este gigante - que dice - Pues que este tiempo que dice Sancho - Pues , seor don Quijote -, que dice - que dice Sancho -, que dice Sancho - dice Sancho - respondi Sancho - respondi Sancho Panza - respondi Sancho Panza , que dice Sancho Panza dice que dice Sancho Panza -, que dice Sancho Panza , Sancho - respondi Sancho - respondi Sancho - Digo que dice Sancho Panza -, que Sancho Panza - respondi Sancho Panza - respondi Sancho - respondi Sancho - respondi Sancho Panza - respondi Sancho Panza , que Sancho Panza - respondi Sancho Panza -, que dice que fue una hermosa Dorotea -, que Sancho - que dice Sancho - que Sancho - que Sancho - respondi Sancho - respondi Sancho - respondi Sancho -, que dice Sancho , que Sancho -, que dice Sancho - Pues , que dice Sancho -, que fuere servido - que fue Sancho - respondi Sancho - respondi Sancho -, que dice Sancho - respondi Sancho - replic Sancho -, que Sancho Panza - respondi Sancho -, que fue la cual fue el cual fue llamada Dorotea -,\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 06: 100%|| 10516/10516 [26:28<00:00,  6.62it/s, loss=5.036]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: You don't hesitate to take a place at my side, do you?\n",
            "    TARGET: Acaso teme sentarse a mi lado?\n",
            " PREDICTED:  No os place ,  No os habis hecho usted a mi lado ,  No os habis venido a vos ?\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: In the middle of the night, and under all the rest of our distresses, one of the men that had been down to see cried out we had sprung a leak; another said there was four feet water in the hold.\n",
            "    TARGET: A medianoche, y para colmo de nuestras desgracias, uno de los hombres que haba bajado a ver la situacin, grit que tenamos una grieta y otro dijo que tenamos cuatro pies de agua en la bodega.\n",
            " PREDICTED: En medio de ver que se haba sido una noche , el cual , que se haba sido una noche , que haba sido una noche , que haba estado de ver que se haba estado de ver a ver que haba estado de ver que haba estado de ver a ver que haba estado de la noche , que haba estado de la noche , que haba estado de la noche , se haba estado de la noche , y de la noche , que se haba estado de la noche , y el cual haba estado de la noche ! dijo que haba sido una noche , que haba estado de la noche .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 07: 100%|| 10516/10516 [26:25<00:00,  6.63it/s, loss=3.972]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: 'Some day a son may be born, my son, and he will by law be a Karenin, and not heir either to my name or my property, and however happy we may be in our family life, and whatever children we may have, there will be no legal bond between them and me.\n",
            "    TARGET: Alexey continu: Maana podemos tener un hijo. Por la naturaleza ser hijo mo; por la ley, ser Karenin, y no podr ser el heredero de mi fortuna.\n",
            " PREDICTED:  Mi hijo mo , y mi nombre y puede ser mi nombre y mi nombre y mi hijo y mi nombre y mi nombre y a mi nombre y , y mi nombre y mi nombre y mi nombre y mi nombre y mi nombre y mi nombre y mi nombre , hijo mo , y mi cuado y mi nombre y mi nombre y mi nombre y mi vida , y mi nombre y no puede ser mi hijo mo y mi nombre y mi nombre y mi vida y mi vida y mi vida y mi vida ; y no puede ser mi nombre y mi nombre y mi vida y mi nombre y mi nombre y mi hijo mo y mi vida y mi nombre de mi nombre , y mi nombre y mi nombre y el hijo mo y mi nombre de mi vida y mi vida y mi vida , y mi vida y el heredero hijos mos y mi vida , y mi vida , y mi nombre y mi hijo mo y mi nombre y mi nombre y mi nombre y mi nombre y mi hijo mo ''.\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: But enough  enough about it!' and he got up.\n",
            "    TARGET: Bien; basta. No hablemos ms de eso aadi, levantndose.\n",
            " PREDICTED: Pero , todo esto , todo esto !  Y , todo esto !  Pero , todo esto !  Pero ,  Pero ,  Y qu es necesario !  Pero ,  Pero esto !\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 08: 100%|| 10516/10516 [26:26<00:00,  6.63it/s, loss=4.538]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I am going through the City first, and we can have some lunch on the way.\n",
            "    TARGET: Antes tengo que pasar por la City, y podemos comer algo por el camino.\n",
            " PREDICTED: A propsito de volver a travs de volver a travs de volver a travs de volver a travs de volver a travs de volver a travs de volver a travs de llegar a travs de llegar a travs de la City .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: WHICH TREATS OF THE EXALTED ADVENTURE AND RICH PRIZE OF MAMBRINO'S HELMET, TOGETHER WITH OTHER THINGS THAT HAPPENED TO OUR INVINCIBLE KNIGHT\n",
            "    TARGET: Que trata de la alta aventura y rica ganancia del yelmo de Mambrino, con otras cosas sucedidas a nuestro invencible caballero\n",
            " PREDICTED: Que trata de la aventura de la aventura de la aventura de la aventura de la de la aventura de la aventura de la historia\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 09: 100%|| 10516/10516 [26:29<00:00,  6.62it/s, loss=4.436]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Je prfere de beaucoup loger avec un camarade que vivre tout seul. \n",
            "    TARGET: El joven Stamford, el vaso en la mano, me mir de forma un tanto extraa.\n",
            " PREDICTED: En todo lo largo tiempo para m mismo tiempo que he visto en el tiempo para m .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"There goes one!\" cried Athos, at the end of five hundred paces.\n",
            "    TARGET: Unol dijo Athos al cabo de quinientos pasos.\n",
            " PREDICTED:  Y Athos ! exclam Athos ! exclam Athos ! exclam Athos ! exclam Athos ! exclam Athos ! exclam Athos ! exclam Athos !  Athos ! exclam Athos ! exclam Athos !\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 10: 100%|| 10516/10516 [26:29<00:00,  6.62it/s, loss=3.909]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: She stood with her figure outlined against the flood of light, one hand upon the door, one half-raised in her eagerness, her body slightly bent, her head and face protruded, with eager eyes and parted lips, a standing question.\n",
            "    TARGET: Permaneci inmvil, con su silueta recortada contra la luz, una mano apoyada en la puerta, la otra a medio alzar en un gesto de ansiedad, el cuerpo ligeramente inclinado, adelantando la cabeza y la cara, con ojos impacientes y labios entreabiertos. Era la estampa viviente misma de la incertidumbre.\n",
            " PREDICTED: Kitty miraba con una mano , con la mano con una sonrisa , con una mano , con una de la cabeza inclinada ligeramente inclinada ligeramente inclinada ligeramente inclinada y con una expresin de la cabeza inclinada ligeramente inclinada ligeramente inclinada ligeramente inclinada ligeramente inclinada ligeramente inclinada ligeramente inclinada ligeramente inclinada sobre la cabeza inclinada sobre la cabeza , con una figura y , con una sonrisa , con una sonrisa , con una sonrisa , con una sonrisa .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Professor, these same instruments are found in my stateroom, where I'll have the pleasure of explaining their functions to you.\n",
            "    TARGET: -Seor profesor, esos instrumentos estn tambin en mi camarote, y es all donde tendr el placer de explicarle su empleo.\n",
            " PREDICTED: - Seor - Seor - Seor - Seor - Seor - Seor , donde estn en el mismo , donde estn en el mismo , donde el mismo tiempo , donde estn en el mo .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 11: 100%|| 10516/10516 [26:30<00:00,  6.61it/s, loss=4.408]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Pshaw, my dear fellow, what do the public, the great unobservant public, who could hardly tell a weaver by his tooth or a compositor by his left thumb, care about the finer shades of analysis and deduction!\n",
            "    TARGET: Ps. Querido amigo, qu le importan al pblico, al gran pblico despistado, que sera incapaz de distinguir a un tejedor por sus dientes o a un cajista de imprenta por su pulgar izquierdo, los matices ms delicados del anlisis y la deduccin?\n",
            " PREDICTED: - Qu cree que qu cree que se lo que por el que se lo que se lo que el que el que el que haba producido por el que por el que el que el que el que el pulgar , que lo que se lo que s mismo tiempo que dice bien el pulgar , que se dice bien conocido por qu sirve para el que lo que dice bien conocido por el pulgar , que es decir que sea el trabajo , que lo que sea el que el arma que el que lo que lo que dice el gusto de su pulgar , que lo que lo que lo que lo que el pulgar , que el ms tiempo que lo que dice lo que es lo que lo que dice que el ms que lo que lo que dice el que no ha sido el que lo que dice que el que el que el tiempo maravilloso volumen de su propietario .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"This is most important,\" said he when I had concluded. \"It fills up a gap which I had been unable to bridge, in this most complex affair.\n",
            "    TARGET: -Todo eso es de gran importancia en este asunto tan complicado -dijo cuando termin-, porque colma una laguna que yo haba sido incapaz de llenar.\n",
            " PREDICTED: - Esto es importante , dijo -, que no es importante -, que no es importante que no poder estar muy importante que no es importante para que no es importante que no es importante que no es importante para que sea importante es importante que no es importante que es importante que no es importante que es importante .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 12: 100%|| 10516/10516 [26:29<00:00,  6.62it/s, loss=4.693]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Come, come, calm yourself, my sweet girl!\n",
            "    TARGET: Mas esto es ms que odio, esto es ingratitud.\n",
            " PREDICTED: Vamos ,  Ven !   Ven !                                            !  !          !    !    !             !   !  !  !        !\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: He wore a long-skirted blue coat with buttons very low down at the back, high boots drawn quite straight over the calves of his legs and crinkled round the ankles, and over them he had on a pair of large goloshes.\n",
            "    TARGET: Tena los ojos saltones y turbios. Vesta una larga levita azul, con botones muy bajos en los faldones, y calzaba botas altas, arrugadas en los tobillos y rectas en las piernas, protegidas por grandes chanclos.\n",
            " PREDICTED: Sali del crculo , pasear con el patio azul , y los grandes saltos y muy alto , muy alta , muy hermosos y all , y muy alto , y botas altas botas altas botas poco de manera muy poco de los pies sobre el suelo muy elegante y muy poco , muy poco de manera muy alto , muy alto y botas altas botas altas botas altas botas altas botas altas botas altas botas poco .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 13: 100%|| 10516/10516 [26:35<00:00,  6.59it/s, loss=4.541]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Then it will be necessary to waken him and take him with us,\" said the renegade, \"and everything of value in this fair mansion.\"\n",
            "    TARGET: ''Pues ser menester despertalle -replic el renegado-, y llevrnosle con nosotros, y todo aquello que tiene de valor este hermoso jardn.''\n",
            " PREDICTED: - dijo el renegado dijo el renegado dijo el renegado dijo : \" dijo Ana -, y dijo : el renegado a s ; y el renegado le dijo : - dijo : - dijo :\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: When Sancho heard his master's firm, resolute language, a cloud came over the sky with him and the wings of his heart drooped, for he had made sure that his master would not go without him for all the wealth of the world; and as he stood there dumbfoundered and moody, Samson Carrasco came in with the housekeeper and niece, who were anxious to hear by what arguments he was about to dissuade their master from going to seek adventures.\n",
            "    TARGET: Cuando Sancho oy la firme resolucin de su amo se le anubl el cielo y se le cayeron las alas del corazn, porque tena credo que su seor no se ira sin l por todos los haberes del mundo; y as, estando suspenso y pensativo, entr Sansn Carrasco y la sobrina, deseosos de or con qu razones persuada a su seor que no tornarse a buscar las aventuras.\n",
            " PREDICTED: Cuando llegaba el don Quijote , y habl de su amo , y el del mundo , y el que no se vio a su seor ; y el corazn de su amo y , y el corazn de su seor don Quijote y el que se su amo , y el corazn de su seor ; y el mundo y el mundo .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 14: 100%|| 10516/10516 [26:32<00:00,  6.60it/s, loss=4.023]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The intensity of these underground forces continues to diminish.\n",
            "    TARGET: La violencia de las fuerzas subterrneas va decreciendo cada vez ms.\n",
            " PREDICTED: La intensidad de aquellas condiciones de la fatiga .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: It's about Isa.\n",
            "    TARGET: Se trata de Isa.\n",
            " PREDICTED: En el cual se haba llegado a la salida del cual se haba llegado a los trabajos .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 15: 100%|| 10516/10516 [26:27<00:00,  6.62it/s, loss=3.657]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: Scissors were made among other things, and the settlers were at last able to cut their hair, and also to shave, or at least trim their beards.\n",
            "    TARGET: Se fabricaron tijeras y los colonos pudieron cortarse el pelo y, si no afeitarse, por lo menos arreglarse la barba.\n",
            " PREDICTED: Los colonos tenan las barbas y entre otras cosas haban podido disparar algunos elementos deban corresponder a los otros haban podido pasar adelante sus vestidos , los otros haban podido ser rica , y otras cosas haban podido hacerlo .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: That day the Nautilus was put to work in some depth-sounding experiments that fascinated me deeply.\n",
            "    TARGET: Aquel da, se someti al Nautilus a diversos experimentos de sondeo que me interesaron vivamente.\n",
            " PREDICTED: Aquel da siguiente , yo me produjo bajo el Nautilus me puse en el Nautilus me puse en aquel da en busca de m me las cercanas del Nautilus .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 16: 100%|| 10516/10516 [26:27<00:00,  6.62it/s, loss=3.623]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: About five ft. seven in. in height; strongly built, sallow complexion, black hair, a little bald in the centre, bushy, black side-whiskers and moustache; tinted glasses, slight infirmity of speech.\n",
            "    TARGET: Estatura, unos cinco pies y siete pulgadas; complexin fuerte, piel atezada, cabello negro con una pequea calva en el centro, patillas largas y bigote negro; gafas oscuras, ligero defecto en el habla.\n",
            " PREDICTED: Hacia las siete segundos negra , con una altura , que se acerc a siete segundos , pelo negro , que una altura , que se acerc a la altura , que negros , que se acerc a la altura , con aspecto de sus siete aos atrs .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Still more; as soon as Monsieur had left and disappeared round the corner of the street, Monsieur Bonacieux took his hat, shut his door, and set off at a quick pace in an opposite direction.\"\n",
            "    TARGET: Adems, tan pronto como el seor le ha dejado y ha desaparecido por la esquina de la calle, el seor Bonacieux ha cogido su sombrero, ha cerrado su puerta y se ha puesto a correr en direccin contraria.\n",
            " PREDICTED: Sin embargo , seor Bonacieux se precipit hacia el seor Bonacieux se march al seor Bonacieux se dej paso , se encontr en el sombrero en la puerta , y desapareci detrs de la vuelta al seor Bonacieux .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 17: 100%|| 10516/10516 [26:31<00:00,  6.61it/s, loss=3.591]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The prisoner turned with the reckless air of a man who abandons himself to his destiny. \"Be it so,\" said he.\n",
            "    TARGET: Lo reconozco por las fotografas! El preso se volvi con el aire indiferente de quien se abandona en manos del destino. De acuerdo dijo.\n",
            " PREDICTED: Sea dijo el destino , que el destino ; el destino , que el destino , que se volvi a su destino ; el destino ; el destino .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: \"Then you have been in Paraguay?\" asked Candide.\n",
            "    TARGET: Con que tu ya has estado en el Paraguay? le dixo Candido.\n",
            " PREDICTED:  Y en el caso ?  pregunt el momento ? - pregunt Candido .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 18: 100%|| 10516/10516 [26:29<00:00,  6.62it/s, loss=3.899]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: And is Philip the gardener really still living?\n",
            "    TARGET: Y la casa? Sigue como antes?\n",
            " PREDICTED: Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y  Y ahora ?\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: DArtagnan fought three times with Rochefort, and wounded him three times.\n",
            "    TARGET: D'Artagnan se bati tres veces con Rochefort y lo hiri tres veces.\n",
            " PREDICTED: D ' Artagnan tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces tres veces .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing epoch 19: 100%|| 10516/10516 [26:30<00:00,  6.61it/s, loss=3.646]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: The information about Mr. John's death and the manner of it came too suddenly: it brought on a stroke.\n",
            "    TARGET: Las prdidas de dinero y el temor a la pobreza la han empeorado. Y la brusca noticia del suicidio del seorito le produjo un ataque.\n",
            " PREDICTED: El seor John apareci una informacin , y se trata de pronto , y la muerte de pronto , y muy poco de pronto como un poco de repente .\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: I have a horror of fallen women.\n",
            "    TARGET: Aborrezco a las mujeres perdidas.\n",
            " PREDICTED:  Y las mujeres , y las mujeres , y las mujeres .\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}